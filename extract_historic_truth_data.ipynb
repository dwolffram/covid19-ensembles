{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yml\", 'r') as ymlfile:\n",
    "    cfg = yaml.safe_load(ymlfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = cfg['access_token']\n",
    "headers = {'Authorization': 'token ' + token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict = {'deaths': 'time_series_covid19_deaths_US.csv',\n",
    "             'cases': 'time_series_covid19_confirmed_US.csv'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips = pd.read_csv('data/locations.csv')\n",
    "fips = fips[fips.location.str.len() <= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in file_dict:\n",
    "    # retrieve information about all commits that modified the file we want\n",
    "    all_commits = []\n",
    "\n",
    "    page = 0\n",
    "    while True:\n",
    "        page += 1\n",
    "        r = requests.get(\n",
    "            'https://api.github.com/repos/CSSEGISandData/COVID-19/commits',\n",
    "            params = {\n",
    "                'path': 'csse_covid_19_data/csse_covid_19_time_series/' + file_dict[target],\n",
    "                'page': str(page)\n",
    "            },\n",
    "            headers=headers\n",
    "        )\n",
    "        \n",
    "        if (not r.ok) or (r.text == '[]'):\n",
    "            break\n",
    "        \n",
    "        all_commits += json.loads(r.text or r.content)\n",
    "\n",
    "    # date of each commit\n",
    "    commit_dates = [\n",
    "        commit['commit']['author']['date'][0:10] for commit in all_commits\n",
    "    ]\n",
    "\n",
    "\n",
    "    \n",
    "    # sha for the last commit made each day\n",
    "    commit_shas_to_get = {}\n",
    "    for index, commit_date in enumerate(commit_dates):\n",
    "        # location in which to save file\n",
    "        result_path =  'data/JHU/raw/' + commit_date + '_JHU_raw_' + target + '.csv'\n",
    "        \n",
    "        # delete file if it was downloaded on the commit date since it may not\n",
    "        # be the last commit that day\n",
    "        if os.path.isfile(result_path):\n",
    "            creation_time = os.path.getctime(result_path)\n",
    "            creation_date = time.strftime(\n",
    "                \"%Y-%m-%d\",\n",
    "                time.gmtime(creation_time)\n",
    "            )\n",
    "            if creation_date == commit_date:\n",
    "                os.remove(result_path)\n",
    "        \n",
    "        # record as a sha to download if applicable\n",
    "        commit_date_as_date = datetime.date(\n",
    "                int(commit_date[0:4]),\n",
    "                int(commit_date[5:7]),\n",
    "                int(commit_date[8:10]))\n",
    "        commit_weekday = commit_date_as_date.weekday()\n",
    "        if (commit_date not in commit_shas_to_get) and \\\n",
    "            (not os.path.isfile(result_path)) and \\\n",
    "            (commit_weekday == 0 or commit_weekday == 6 or\n",
    "                datetime.datetime.today().date() - commit_date_as_date < datetime.timedelta(7)):\n",
    "            commit_shas_to_get[commit_date] = all_commits[index]['sha']\n",
    "\n",
    "    \n",
    "    # download and save the csvs\n",
    "    for commit_date, commit_sha in commit_shas_to_get.items():\n",
    "        df = pd.read_csv(\n",
    "            'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/' +\n",
    "            commit_sha +\n",
    "            '/csse_covid_19_data/csse_covid_19_time_series/' +\n",
    "            file_dict[target])\n",
    "        \n",
    "        result_path =  'data/JHU/raw/' + commit_date + '_JHU_raw_' + target + '.csv'\n",
    "        df.to_csv(result_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(filename, saturdays_only=True):\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    df.drop(columns=['UID', 'iso2', 'iso3', 'code3', 'FIPS', 'Admin2', 'Country_Region', \n",
    "                     'Lat', 'Long_', 'Combined_Key', 'Population'], errors='ignore', inplace=True)\n",
    "\n",
    "    df = df.groupby('Province_State').sum().reset_index()\n",
    "    df = pd.melt(df, id_vars=['Province_State'])\n",
    "    df.columns = ['location_name', 'date', 'value']\n",
    "    df.date = pd.to_datetime(df.date)\n",
    "\n",
    "    df = df.merge(fips[['location', 'location_name']], how='left')\n",
    "    df = df[['date', 'location', 'location_name', 'value']].sort_values(['date', 'location'])\n",
    "    \n",
    "    if saturdays_only:\n",
    "        df = df[df.date.dt.day_name() == 'Saturday'].reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing deaths:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa924fcb7444a88bb22a81f82de2b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cases:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a51fb3af5444d780f8934438e016f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_files = glob.glob('data/JHU/raw/*')\n",
    "\n",
    "for target in ['deaths', 'cases']:\n",
    "    # only keep files with the respective target\n",
    "    files = [f for f in list_of_files if target in f]\n",
    "    \n",
    "    # only consider data from Monday\n",
    "    file_df = pd.DataFrame({'filename': files})\n",
    "    file_df['date'] = file_df.filename.transform(lambda x: x.split('\\\\')[1][:10])\n",
    "    file_df.date = pd.to_datetime(file_df.date)\n",
    "    file_df = file_df[file_df.date.dt.day_name() == 'Monday'].reset_index(drop=True)\n",
    "    \n",
    "    print('Processing ' + target + ':')\n",
    "    for _, row in tqdm(file_df.iterrows(), total=file_df.shape[0]):\n",
    "        temp = process_file(row['filename'], saturdays_only=True)\n",
    "        # temp.dropna(inplace=True) Diamond Princess and Grand Princess are counted for US\n",
    "        temp.drop(columns=['location_name'], inplace=True)\n",
    "\n",
    "        us = temp.groupby('date')['value'].sum().reset_index()\n",
    "        us['location'] = 'US'\n",
    "\n",
    "        temp.dropna(inplace=True) # drop Diamond Princess and Grand Princess\n",
    "        temp = pd.concat([temp, us]).sort_values(['date', 'location']).reset_index(drop=True)\n",
    "\n",
    "        temp.to_csv('data/JHU/cumulative_' + target + '/truth_jhu_cumulative_' + target + '_' + \n",
    "                    str(row['date'].date()) + '.csv', index=False)\n",
    "        \n",
    "        # compute incidence\n",
    "        temp.value = temp.groupby(['location'])['value'].diff()\n",
    "        temp.dropna(inplace=True)\n",
    "        temp.value = temp.value.astype(int)\n",
    "        \n",
    "        temp.to_csv('data/JHU/incident_' + target + '/truth_jhu_incident_' + target + '_' + \n",
    "                    str(row['date'].date()) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
